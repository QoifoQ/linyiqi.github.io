---
permalink: /STAT3009/
title: "STAT3009 Recommender Systems"
---

> *"There is Nothing More Practical Than A Good Theory."" ‚Äî Kurt Lewin*

- [üìù <span style="color:#A04000"> Administrative information </span>](#--administrative-information-)
- [üóìÔ∏è <span style="color:#A04000"> Schedule (tentative) </span>](#Ô∏è--schedule-tentative-)
- [üßæ <span style="color:#A04000"> Course Content </span>](#--course-content-)
- [üíØ <span style="color:#A04000"> Grading (tentative) </span>](#--grading-tentative-)
- [üìã <span style="color:#A04000"> Recommended Readings </span>](#--recommended-readings-)


## üìù <span style="color:#A04000"> Administrative information </span>

- ‚è≤Ô∏è **Lectures**: **Fri**. 10:30AM - 12:15PM
- üë®‚Äçüè´ **Instructor**: [Ben Dai](http://www.bendai.org)
- üë®‚Äçüíº **TA**: [Hao Shi]
- ‚è≥ **Office hours**: **Fri**. 2:00PM - 3:00PM

## üóìÔ∏è <span style="color:#A04000"> Schedule (tentative) </span>

 Week | Content | Slides | Python code | Readings 
 ------- | ------- | ------- | ------- | ------- 
 Week00 | Python Numpy Tutorial || [Numpy tutorial](https://cs231n.github.io/python-numpy-tutorial/) |
 Week01 | Background, dataset, evaluation, metrics, baseline method, and software prepare | [slides](https://www.dropbox.com/s/rtpjy1vak639096/slide.pdf?dl=0) | [notebook](https://github.com/statmlben/CUHK-STAT3009/blob/main/notebook1.ipynb) | Sections 1.1 - 1.3 in [1] 
 Week02 | Correlation-based collaborative filtering | [slides](https://www.dropbox.com/s/ly0dis4emhce903/slide.pdf?dl=0) | [notebook](https://github.com/statmlben/CUHK-STAT3009/blob/main/notebook2.ipynb) | Sections 2.1 - 2.3 in [1] 
 | `Quizz 1`: implement for baseline methods and Correlation-based collaborative filtering || [Kaggle InClass Prediction Competition](https://www.kaggle.com/c/cuhkrecsys1/)
 Week03 | Latent factor model I: methodology | [slides](https://www.dropbox.com/s/akzet5mn3c7u0x3/slide.pdf?dl=0) | [notebook](https://github.com/statmlben/CUHK-STAT3009/blob/main/notebook3.ipynb) | [3], [7], Section 3.6 in [1] 
 Week04 | Latent factor model II: alternating least square (ALS) | [slides](https://www.dropbox.com/s/gcxwl9pxwwjkfeg/slide.pdf?dl=0) | [notebook](https://github.com/statmlben/CUHK-STAT3009/blob/main/notebook4.ipynb) | Section 3.6.4.4 in [1], [8] 
 Week05 | Latent factor model III: stochastic gradient descent (SGD) | [slides](https://www.dropbox.com/s/o1qjt3z8vq8x1qe/slide.pdf?dl=0) | [notebook](https://github.com/statmlben/CUHK-STAT3009/blob/main/notebook5.ipynb) | Section 3.6.4.1, [14], [15] 
 Week06 | Break
 Week07 | Smooth Latent Factor Models | [slides](https://www.dropbox.com/s/d9caphsyc9ok1uf/slide.pdf?dl=0) | [notebook](https://github.com/statmlben/CUHK-STAT3009/blob/main/notebook6.ipynb) | Section 1.5 in [1] 
 Week08 | Case study: MovieLens dataset |  | [notebook](https://github.com/statmlben/CUHK-STAT3009/blob/main/notebook7.ipynb) | [16], [17] 
  | `Project 1`: Recommender systems based on LFM || [Kaggle InClass Prediction Competition](https://www.kaggle.com/c/cuhk-stat3009-recommender-systems-proj-1) 
 Week09 | Neural collaborative filtering: nonlinear interaction | [slides](https://www.dropbox.com/s/zyd92x9gmfgxa1u/slide.pdf?dl=0) | [notebook](https://github.com/statmlben/CUHK-STAT3009/blob/main/notebook8.ipynb) | [2], [5], [19]
 Week10 | Side information: continuous and discrete features | [slides](https://www.dropbox.com/s/6z154pc9zcd4zwz/slide.pdf?dl=0) | [notebook](https://github.com/statmlben/CUHK-STAT3009/blob/main/notebook9.ipynb) | [2], [10] 
 Week11 | binary recommender systems and Top-K recommendation | [slides1](https://www.dropbox.com/s/ro9ht8dcp9nwwma/slide.pdf?dl=0)+[slides2](https://www.dropbox.com/s/9luhdrinxshsdvx/slide.pdf?dl=0) | [notebook](https://github.com/statmlben/CUHK-STAT3009/blob/main/notebook10a.ipynb) | Section 10.4 in [1], [11], [12] 
 | `Project 2`: Recommender system based on Real application || [Kaggle InClass Prediction Competition](https://www.kaggle.com/c/cuhk-stat3009-recommender-systems-proj-2)
 Week12 | Advance topic: convolutional neural networks based Recommendation | [slides]() | [notebook]() | [2], [13]
 Week13 | Advance topic: Causality in recommender systems | [slides](https://www.dropbox.com/s/r65gfgra5z1thld/slides.pdf?dl=0) |  | [18]
 | `Quizz 2`: Inclass exercise for statistical modelling (similar to the homework)
 Week14 | Top-3 solution presentation

## üßæ <span style="color:#A04000"> Course Content </span> 

üñ•Ô∏è **Description:**

This course will provide tools to the theoretical analysis of statistical machine learning methods. It will cover approaches such as parametric models, neural networks, kernel methods, SVM to tasks such as regression, classifiaction, recommender systems, ranking, and it will focus on developing a theoretical understanding and insights of the statistical properties of learning methods. 

üîë **Key words:**

- Empirical risk minimization
- Estimation error, approximation error
- Regret or excess risk bounds, convergence rate, consistency
- Fisher consistency, calibrated surrogate losses
- Uniform concentration inequality
- Rademacher complexity, covering number, entropy
- Penalization, method of sieve
- Local/random complexity

üèóÔ∏è **Prerequisites:**

- Probability at the level of STAT5005 or equivalent (plus mathematical maturity). This is an advanced theory course, a strong mathematical/statistical/probabilistic background is necessary.


## üíØ <span style="color:#A04000"> Grading (tentative) </span>

üë®‚Äçüíª **Coursework:**

- Homeworks (50%)

There will be three homework assignments. You are welcome to discuss Problems with other students, but the final solutions should be completely on your own. You will receive one bonus point for a typed written assignment in LaTeX or Markdown. We will accept scanned handwritten version but without the bonus point. **Late submission will not be accepted.**


- Paper review / project (50%)

You will write a review of 2-3 papers on the same topic, which can be in any area related to the course. (1) You should summarize and critique the *assumptions* and *theoretical results* in the papers, discuss its overall *contributions*. (2) You might extend a theoretical result, develop a new method and investigate its performance, or run experiments to see the applicability of the methods.
It is OK to work on projects in groups of three, see **Collaboration policy**.


üë®üèª‚Äçü§ù‚Äçüë®üèæ **Collaboration policy**: we admit you to form a group to finish your final project. The number of group members should be smaller or equal than 3. The contribution of each member should be clearly stated in the final report. You will receive one (1) bonus point if you work solo to projects.

## üìã <span style="color:#A04000"> Textbooks </span>

1. Koltchinskii, V. (2011). *Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems: Ecole d‚ÄôEt√© de Probabilit√©s de Saint-Flour* XXXVIII-2008 (Vol. 2033). Springer Science & Business Media.

2. Van Der Vaart, A. W., van der Vaart, A. W., van der Vaart, A., & Wellner, J. (1996). *Weak Convergence and Empirical Processes: with Applications to Statistics*. Springer Science & Business Media.

3. Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.

4. Anthony, M., Bartlett, P. L., & Bartlett, P. L. (1999). *Neural network learning: Theoretical foundations* (Vol. 9). Cambridge: cambridge university press.



## üßæ <span style="color:#A04000"> Reference course </span>

1. Peter Bartlett, CS 281B / Stat 241B: [Statistical Learning Theory](https://bcourses.berkeley.edu/courses/1409209)

2. Tengyu Ma, STATS214 / CS229M: [Machine Learning Theory](http://web.stanford.edu/class/stats214/)

3. Larry Wasserman, 36-708: [Statistical Methods for Machine Learning](http://www.stat.cmu.edu/~larry/=sml/)

4. Clayton Scott, EECS 598: [Statistical Learning Theory](http://web.eecs.umich.edu/~cscott/past_courses/eecs598w14/)

5. Yoonkyung Lee, STAT 881: [Advanced Statistical Learning](https://www.asc.ohio-state.edu/lee.2272/)