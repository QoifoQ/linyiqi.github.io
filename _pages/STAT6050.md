---
permalink: /STAT6050/
title: "STAT6050 Statistical Learning Theory"
---

> *"There is Nothing More Practical Than A Good Theory." â€” Kurt Lewin*

- [ğŸ“ <span style="color:#A04000"> Administrative information </span>](#--administrative-information-)
- [ğŸ—“ï¸ <span style="color:#A04000"> Schedule (tentative) </span>](#ï¸--schedule-tentative-)
- [ğŸ§¾ <span style="color:#A04000"> Course Content </span>](#--course-content-)
- [ğŸ’¯ <span style="color:#A04000"> Grading (tentative) </span>](#--grading-tentative-)
- [ğŸ“‹ <span style="color:#A04000"> Textbooks </span>](#--textbooks-)
- [ğŸ§¾ <span style="color:#A04000"> Reference course </span>](#--reference-course-)


## ğŸ“ <span style="color:#A04000"> Administrative information </span>

- â²ï¸ **Lectures**: **Fri**. 10:30AM - 12:15PM
- ğŸ« **Room**: Lady Shaw Bldg C5, CUHK
- ğŸ‘¨â€ğŸ« **Instructor**: [Ben Dai](http://www.bendai.org)
- ğŸ‘¨â€ğŸ’¼ **TA**: [Hao Shi](https://www.sta.cuhk.edu.hk/peoples/shi-hao/)
- â³ **Office hours**: **Fri**. 2:00PM - 3:00PM

## ğŸ—“ï¸ <span style="color:#A04000"> Schedule (tentative) </span>

 Week | Content
 ------- | ------- | 
 Week01 | [Introduction](/STAT6050_slides/lecture1.pdf) | 
 Week02 | [Approximation error and estimation error](/STAT6050_slides/lecture2.pdf)
 Week03 | [Uniform concentration inequality](/STAT6050_slides/lecture3.pdf)
 Week04 | [Rademacher complexity I](/STAT6050_slides/lecture4.pdf)
 Week05 | [Rademacher complexity II](/STAT6050_slides/lecture5.pdf)
 Week06 | [Method of regularization](/STAT6050_slides/lecture6.pdf)
 Week07 | [Nonparametric regression on RKHS](/STAT6050_slides/lecture7.pdf)
 Week08 | [Classification: Fisher consistency and calibrated surrogate losses](/STAT6050_slides/lecture8.pdf)
 Week09 | [Revisiting Excess Risk Bounds: chain argument]
 Week10 | [Revisiting Excess Risk Bounds: local complexity and random entropy]
 Week11 | [Case study: recommender systems]
 Week12 | [Case study: ranking]
 Week13 | [Case study: neural networks]

## ğŸ§¾ <span style="color:#A04000"> Course Content </span> 

ğŸ–¥ï¸ **Description:**

This course will provide tools to the theoretical analysis of statistical machine learning methods. It will cover approaches such as parametric models, neural networks, kernel methods, SVM to tasks such as regression, classifiaction, recommender systems, ranking, and it will focus on developing a theoretical understanding and insights of the statistical properties of learning methods. 

ğŸ”‘ **Key words:**

- Empirical risk minimization
- Estimation error, approximation error
- Regret or excess risk bounds, convergence rate, consistency
- Fisher consistency, calibrated surrogate losses
- Uniform concentration inequality
- Rademacher complexity, covering number, entropy
- Penalization, method of sieve
- Local/random complexity

ğŸ—ï¸ **Prerequisites:**

- Probability at the level of STAT5005 or equivalent (plus mathematical maturity). This is an advanced theory course, a strong mathematical/statistical/probabilistic background is necessary.


## ğŸ’¯ <span style="color:#A04000"> Grading (tentative) </span>

ğŸ‘¨â€ğŸ’» **Coursework:**

- Homeworks (50%)

There will be three homework assignments. You are welcome to discuss Problems with other students, but the final solutions should be completely on your own. You will receive one bonus point for a typed written assignment in LaTeX or Markdown. We will accept scanned handwritten version but without the bonus point. **Late submission will not be accepted.**


- Paper review / project (50%)

You will write a review of 2-3 papers on the same topic, which can be in any area related to the course. (1) You should summarize and critique the *assumptions* and *theoretical results* in the papers, discuss its overall *contributions*. (2) You might extend a theoretical result, develop a new method and investigate its performance, or run experiments to see the applicability of the methods.
It is OK to work on projects in groups of three, see **Collaboration policy**.


ğŸ‘¨ğŸ»â€ğŸ¤â€ğŸ‘¨ğŸ¾ **Collaboration policy**: we admit you to form a group to finish your final project. The number of group members should be smaller or equal than 3. The contribution of each member should be clearly stated in the final report. You will receive one (1) bonus point if you work solo to projects.

## ğŸ“‹ <span style="color:#A04000"> Textbooks </span>

1. Koltchinskii, V. (2011). *Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems: Ecole dâ€™EtÃ© de ProbabilitÃ©s de Saint-Flour* XXXVIII-2008 (Vol. 2033). Springer Science & Business Media.

2. Van Der Vaart, A. W. & Wellner, J. (1996). *Weak Convergence and Empirical Processes: with Applications to Statistics*. Springer Science & Business Media.

3. Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.

4. Anthony, M., & Bartlett, P. L. (1999). *Neural network learning: Theoretical foundations* (Vol. 9). Cambridge: cambridge university press.


## ğŸ§¾ <span style="color:#A04000"> Reference course </span>

1. Peter Bartlett, CS 281B / Stat 241B: [Statistical Learning Theory](https://bcourses.berkeley.edu/courses/1409209)

2. Tengyu Ma, STATS214 / CS229M: [Machine Learning Theory](http://web.stanford.edu/class/stats214/)

3. Larry Wasserman, 36-708: [Statistical Methods for Machine Learning](http://www.stat.cmu.edu/~larry/=sml/)

4. Clayton Scott, EECS 598: [Statistical Learning Theory](http://web.eecs.umich.edu/~cscott/past_courses/eecs598w14/)

5. Yoonkyung Lee, STAT 881: [Advanced Statistical Learning](https://www.asc.ohio-state.edu/lee.2272/)